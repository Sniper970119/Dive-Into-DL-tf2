{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "2.1.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[0.],\n       [1.],\n       [2.],\n       [3.]], dtype=float32)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "x = tf.reshape(tf.constant(range(4),dtype=tf.float32),(4,1))\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[ 0.],\n       [ 4.],\n       [ 8.],\n       [12.]], dtype=float32)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = 2 * tf.matmul(tf.transpose(x),x)\n",
    "\n",
    "dy_dx = t.gradient(y,x)\n",
    "dy_dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n array([[  0.],\n        [  4.],\n        [ 32.],\n        [108.]], dtype=float32)>,\n <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n array([[0.],\n        [2.],\n        [4.],\n        [6.]], dtype=float32)>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "    dz_dx = g.gradient(z,x)\n",
    "    dy_dx = g.gradient(y,x)\n",
    "dz_dx, dy_dx\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100*b\n",
    "    return c\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[ True]])>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "a = tf.random.normal((1,1),dtype=tf.float32)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(a)\n",
    "    c = f(a)\n",
    "t.gradient(c,a)==c/a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}